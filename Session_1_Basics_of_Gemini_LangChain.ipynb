{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranav4DataSC/Agentic/blob/main/Session_1_Basics_of_Gemini_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Calling Google Gemini Model and LangChain (LCEL)"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install langchain-google-genai -q\n",
        "!pip install langchain-community -q\n",
        "!pip install google-generativeai -q"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97127a0b-ab74-449e-c63c-274d101edb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 3.0.0 requires google-ai-generativelanguage<1.0.0,>=0.7.0, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from the secret key so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "# Configure API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "gemini_model = genai.GenerativeModel(model_name=\"gemini-2.0-flash-lite-001\")"
      ],
      "metadata": {
        "id": "3Pswa8xvcgH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "response = gemini_model.generate_content(\"What is the capital of India?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "reAkGri7cjXa",
        "outputId": "782326cd-c58e-4bf9-f689-be33e60508fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of India is **New Delhi**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbSFoz3Lp0pJ",
        "outputId": "05bdc258-cc9e-4188-f9fc-993a3f03a08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"The capital of France is **Paris**.\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.015217953258090548\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 7,\n",
              "        \"candidates_token_count\": 9,\n",
              "        \"total_token_count\": 16\n",
              "      },\n",
              "      \"model_version\": \"gemini-2.0-flash-lite-001\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gemini-2.0-flash-lite-001\"\n",
        "\n",
        "gemini_model = genai.GenerativeModel(model_id)"
      ],
      "metadata": {
        "id": "Vv2RMwSSdT8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of India?\"\n",
        "\n",
        "response = gemini_model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fCq3kHzFdhVF",
        "outputId": "5c2188e3-aeaf-4ffc-c805-7648fa436754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of India is **New Delhi**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "  gemini_model = genai.GenerativeModel(model_id)\n",
        "  response = gemini_model.generate_content(prompt)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "a48DyrIXrB1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"What is the capital of India?\")"
      ],
      "metadata": {
        "id": "DphnhQNjrTOB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58a131ad-0b5a-4718-9efd-22c73b28e28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of India is **New Delhi**.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=model_id,\n",
        "                                      convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about GenAI\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvLbyyiDeUv_",
        "outputId": "2cdc5419-6e9d-4770-eb7f-dc4e22b0c801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI refuse to write a sonnet?\n",
            "\n",
            "Because it said, \"I'm great at generating, but I just can't rhyme... **I'm a little poem-blematic!**\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JHeZWoQIqbH",
        "outputId": "5b8fd363-a5d4-4fcd-aae9-7545120e03b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the GenAI refuse to write a sonnet?\\n\\nBecause it said, \"I\\'m great at generating, but I just can\\'t rhyme... **I\\'m a little poem-blematic!**\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--a7aec09a-d64a-4000-a2ee-90d02681b6e3-0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=model_id,\n",
        "                                      convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"GenAI\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvEUaoxxmq5Z",
        "outputId": "682bbdcf-c96d-4855-bb0d-b34adcdc7099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI refuse to write a poem about a cat?\n",
            "\n",
            "Because it couldn't decide whether to use a \"purrfect\" rhyme scheme or a \"claw-some\" narrative!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=model_id,\n",
        "                                      convert_system_message_to_human=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "responses = chain.map().invoke([{\"topic\":\"GenAI\"},{\"topic\":\"Mumbai\"}])"
      ],
      "metadata": {
        "id": "JRUMbtFSf7pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3I18ndtqHQ-",
        "outputId": "a115a844-643a-43ed-a2d5-44183e2e73da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content=\"Why did the GenAI refuse to write a poem about cheese?\\n\\nBecause it couldn't get past the *gouda* puns!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--dddd3b3b-f478-4ad6-9980-b9d503722ccd-0'),\n",
              " AIMessage(content='Why did the auto rickshaw driver in Mumbai get a promotion?\\n\\nBecause he was the best at **weaving through the traffic like a pro, dodging everything, and still getting you there... eventually!**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite-001', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--f6a28f85-8a9a-4ec5-a442-0cc06284e29f-0')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print(\"----------------------------------\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h_3m_9GqXpR",
        "outputId": "63685165-635c-4553-a2db-3bd1e61edd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the GenAI refuse to write a poem about cheese?\n",
            "\n",
            "Because it couldn't get past the *gouda* puns!\n",
            "----------------------------------\n",
            "\n",
            "\n",
            "Why did the auto rickshaw driver in Mumbai get a promotion?\n",
            "\n",
            "Because he was the best at **weaving through the traffic like a pro, dodging everything, and still getting you there... eventually!**\n",
            "----------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Complex prompts with placeholders"
      ],
      "metadata": {
        "id": "xV-cqRgBq33i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template using ChatPromptTemplate.from_template()\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Explain to me what is {topic} in 500 words like you would do to a {audience}?\"\n",
        ")\n",
        "\n",
        "# Your input data\n",
        "input_data = [\n",
        "    {\"topic\": \"Generative AI\", \"audience\": \"Child\"},\n",
        "    {\"topic\": \"Recommendation Engine\", \"audience\": \"Senior Citizen\"},\n",
        "    {\"topic\": \"Quantum Physics\", \"audience\": \"GenZ Adult\"}\n",
        "]\n",
        "\n",
        "# Generate prompts for each input using list comprehension:\n",
        "prompts = [prompt_template.format_messages(topic=data[\"topic\"], audience=data[\"audience\"]) for data in input_data]\n",
        "\n",
        "# Generate prompts for each input using for loop\n",
        "# prompts = []\n",
        "# for data in input_data:\n",
        "#     prompt = prompt_template.format_messages(topic=data[\"topic\"], audience=data[\"audience\"])\n",
        "#     prompts.append(prompt)\n",
        "\n",
        "# Display the prompts\n",
        "for prompt in prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpfAbKWiqptc",
        "outputId": "d56e28d6-776e-490f-c8d9-0fe2da5653b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: [HumanMessage(content='Explain to me what is Generative AI in 500 words like you would do to a Child?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n",
            "Prompt: [HumanMessage(content='Explain to me what is Recommendation Engine in 500 words like you would do to a Senior Citizen?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n",
            "Prompt: [HumanMessage(content='Explain to me what is Quantum Physics in 500 words like you would do to a GenZ Adult?', additional_kwargs={}, response_metadata={})]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = chain.map().invoke(prompts)"
      ],
      "metadata": {
        "id": "IPEcB974s3RV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e68add-0e6a-40cd-9d3f-98ee9389cd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print(\"-\" * 50)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4wJpezltINJ",
        "outputId": "da0a1dcd-0a00-47d3-8cdf-b0dd775c56b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a joke about explaining Generative AI to a child:\n",
            "\n",
            "Why did the computer get sad?\n",
            "\n",
            "... Because it couldn't *generate* enough giggles! It was trying to make jokes, but it kept making the same ones over and over! It needed Generative AI to help it come up with some new, funny stuff!\n",
            "\n",
            "---\n",
            "\n",
            "And since you asked me to explain Generative AI to a child, here's that explanation, too!\n",
            "\n",
            "Imagine you have a box of LEGOs. You can use those LEGOs to build a car, a house, a spaceship, or anything you can imagine!\n",
            "\n",
            "Generative AI is like a super-smart LEGO box! Except instead of LEGOs, it has all sorts of information: words, pictures, music, and even code!\n",
            "\n",
            "This super-smart box is really good at learning. It looks at all the information it has and figures out how things are put together. For example:\n",
            "\n",
            "*   **Words:** It learns how sentences are made, what words usually go together, and how stories are told.\n",
            "*   **Pictures:** It learns about shapes, colors, and how to draw different things like animals, people, or landscapes.\n",
            "*   **Music:** It learns about notes, rhythms, and how songs are written.\n",
            "\n",
            "Now, the really cool part! Once the super-smart box has learned, you can tell it to *create* something new, just like you can build something new with your LEGOs!\n",
            "\n",
            "For example, you could tell it:\n",
            "\n",
            "*   \"Make me a picture of a fluffy pink unicorn riding a rainbow.\" And it would try to draw that for you!\n",
            "*   \"Write a story about a brave little robot who saves the world.\" And it would write a story!\n",
            "*   \"Compose a song that sounds like a happy birthday song but is about going to the zoo.\" And it would make the song!\n",
            "\n",
            "That's why we call it \"Generative\" AI. It *generates* or *creates* new things! It's like a magical helper that can make all sorts of things from the information it has learned.\n",
            "\n",
            "Sometimes, the things it makes are perfect! Sometimes, they're a little silly or not quite right, just like when you're building with LEGOs and you accidentally put a wheel on top of a chimney! But it's always learning and getting better, and it's super fun to play with! It's like having a super-powered imagination in a computer!\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Alright, settle in, dearie, and let's talk about this \"Recommendation Engine\" thing. Think of it like this: you're at a delightful little bakery, right? You go in for a nice sourdough, but the baker, bless her heart, also points out the warm apple pie just coming out of the oven. That's a simple recommendation, based on what you *might* like, beyond what you initially asked for.\n",
            "\n",
            "Now, imagine that bakery is the internet, and it's got *everything* – every bread, every cake, every book, every movie. That's a lot, isn't it? You’d be wandering around for days, just browsing!\n",
            "\n",
            "That’s where this “Recommendation Engine” comes in. It's like having a very clever, very *helpful* assistant in that giant internet bakery. This assistant doesn't have a physical body, it's all done with computers and clever programming, but it *watches* you, in a good way, and learns what you like.\n",
            "\n",
            "Think of it like this:\n",
            "\n",
            "*   **You tell it what you like:** You look at a specific sourdough bread on the internet. The assistant takes note. You buy a book about gardening. The assistant makes another note.\n",
            "\n",
            "*   **It learns your tastes:** The more you browse, the more the assistant understands what you *actually* like. It sees you spend a lot of time looking at recipes with apples, and you've bought three gardening books.\n",
            "\n",
            "*   **It suggests things you *might* like:** Based on all this information, the assistant might suggest:\n",
            "    *   \"Since you like sourdough, have you considered this rye bread?\"\n",
            "    *   \"Based on your interest in gardening, you might enjoy this book on wildflowers.\"\n",
            "    *   \"Oh, and by the way, we have a special on apple crumble this week...\" (That's the online equivalent of the baker's apple pie!)\n",
            "\n",
            "*   **It's all about patterns:** The assistant isn't guessing. It's looking for patterns. It notices that other people who bought the same sourdough *also* bought a specific type of cheese. Therefore, it might suggest that cheese to *you*. It's all about finding connections and similarities.\n",
            "\n",
            "*   **It's personalized!** The beauty of this engine is that it's tailored to *you*. Your assistant is different from your neighbor's assistant. Your recommendations are based on *your* preferences, not just random suggestions.\n",
            "\n",
            "Now, it's not perfect. Sometimes, it might make a mistake. Maybe it suggests something you *don't* like. But the more you use the system, the better it gets at figuring out what you'll enjoy.\n",
            "\n",
            "So, next time you see those \"You might also like...\" or \"Customers who bought this also bought...\" suggestions, you know you're seeing the work of that clever little \"Recommendation Engine.\" It's just trying to help you find the things you'll truly enjoy, just like a friendly baker pointing out the best apple pie. It's all about making your internet experience a little easier and a little more delightful, dearie!\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "Okay, here's a joke about quantum physics, tailored for a Gen Z audience:\n",
            "\n",
            "Why did the quantum physicist break up with the photon?\n",
            "\n",
            "...Because they just couldn't **commit**!  They were always in a state of superposition, existing in multiple relationships at once.  Plus, the photon was constantly disappearing and reappearing, like a flakey ex-BFF who ghosts you after promising to hang out.  And honestly, the physicist needed someone who wasn't so... *wavey*.  They wanted someone more solid, you know? Someone who had a defined location and a clear direction, unlike that photon that was always, like, \"maybe I'm here, maybe I'm there, who knows?\"  It was just too much uncertainty for a relationship.  Plus, the photon never paid rent!  It always went *poof* before the bills arrived.\n",
            "--------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11tZzR3GlogU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}